# Aider Configuration for Local Ollama Usage

# Use the Qwen 2.5 Coder model (highly recommended for local coding)
# If you prefer another model, change 'qwen2.5-coder' to your preferred Ollama tag
model: ollama:qwen3-coder:latest
weak-model: ollama:gemma3:latest

# Specify the local Ollama API endpoint
openai-api-base: http://127.0.0.1:11434

# Since we are local, we don't need an API key, but Aider requires a placeholder
openai-api-key: "not-needed"

# Performance and UI Preferences
edit-format: whole
stream: true
dark-mode: true
restore-chat-history: true

# Git Settings
auto-commits: true
dirty-commits: false

# Suggested: Increase the context window for Ollama 
# (Helps with larger files or design patterns)
map-tokens: "4096"
